{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY0sVVaROQ1y",
        "outputId": "0b3483c9-9d99-43bd-9b52-306f4d0dc66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "Extracting GloVe embeddings...\n",
            "GloVe embeddings are ready!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "# Download and extract GloVe embeddings\n",
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "glove_zip_file = 'glove.6B.zip'\n",
        "\n",
        "if not os.path.exists(glove_zip_file):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    urllib.request.urlretrieve(glove_url, glove_zip_file)\n",
        "\n",
        "glove_embedding_file = 'glove.6B.300d.txt'\n",
        "\n",
        "if not os.path.exists(glove_embedding_file):\n",
        "    print(\"Extracting GloVe embeddings...\")\n",
        "    with zipfile.ZipFile(glove_zip_file, 'r') as z:\n",
        "        z.extractall()\n",
        "\n",
        "print(\"GloVe embeddings are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Embedding, TimeDistributed, Lambda, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, Flatten, GRU, MaxPooling1D, Concatenate, Attention, GlobalAveragePooling1D, MultiHeadAttention, Dropout, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "OcW0GJjLkL0T"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAlimUNCkeuA",
        "outputId": "23bc4d74-0404-4570-bc95-1543296b93ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoKbqA9fbKY_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et-HSt3r_ik6",
        "outputId": "987dfefd-c455-4c79-f9fe-7e1702c414b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH0BoMSPQu7o",
        "outputId": "4cbf389f-ec00-419b-d51d-446c1e41938b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and preprocess the data\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    \n",
        "    # Remove non-ASCII characters and convert to lowercase\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode().lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords and stem the remaining words\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    \n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def load_and_preprocess_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    data['Text'] = data['Text'].apply(clean_text)\n",
        "    return data\n",
        "\n",
        "\n",
        "data = load_and_preprocess_data('/content/drive/MyDrive/new_data.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "oS9ASn87OVNe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Embedding, TimeDistributed, Lambda\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "metadata": {
        "id": "yDN3OFv8rMHm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define num_classes and encode labels\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "x9fSwNy6OVPz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenize and pad sequences\n",
        "max_features = 10000\n",
        "max_length = 100\n",
        "\n",
        "# Custom filter to remove special characters\n",
        "custom_filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=max_features,\n",
        "    filters=custom_filters,\n",
        "    lower=True,\n",
        "    split=\" \",\n",
        "    char_level=False, # Set to True for character-level tokenization\n",
        "    oov_token=\"<OOV>\", # Out-of-vocabulary token for words not in the training data\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Save tokenizer for future use (e.g., when preprocessing new data)\n",
        "import pickle\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "w6Y4XJH3OVSU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load GloVe pre-trained word embeddings\n",
        "def load_glove_embeddings(embedding_file, tokenizer):\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_file, encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = load_glove_embeddings('glove.6B.300d.txt', tokenizer)\n"
      ],
      "metadata": {
        "id": "QmE2h2G3OVUm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Build and train different deep learning models\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, TimeDistributed, Flatten, GRU, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, TimeDistributed, Flatten, MaxPooling1D, Concatenate, Attention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "# Transformer model\n",
        "def create_transformer_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    transformer_block = MultiHeadAttention(num_heads=8, key_dim=300)(embedding_layer, embedding_layer)\n",
        "    x = transformer_block\n",
        "    for _ in range(2):\n",
        "        x = MultiHeadAttention(num_heads=8, key_dim=300)(x, x)\n",
        "    \n",
        "    pooling_layer = GlobalAveragePooling1D()(x)\n",
        "    dense_layer = Dense(64, activation='relu')(pooling_layer)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "  \n",
        "\n",
        "# CRNN model\n",
        "def create_crnn_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    conv1d_layer = Conv1D(256, 5, activation='relu')(embedding_layer)\n",
        "    max_pooling_layer = MaxPooling1D(pool_size=2)(conv1d_layer)\n",
        "    gru_layer1 = GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(max_pooling_layer)\n",
        "    gru_layer2 = GRU(128, dropout=0.3, recurrent_dropout=0.3)(gru_layer1)\n",
        "    dense_layer1 = Dense(64, activation='relu')(gru_layer2)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer1)\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Hierarchical Attention Network (HAN) model\n",
        "def create_han_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(embedding_layer)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    concatenated = Concatenate()([avg_pool, max_pool])\n",
        "    output_layer = Dense(num_classes, activation='softmax')(concatenated)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "# TextCNN model\n",
        "def create_textcnn_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    conv_blocks = []\n",
        "    filter_sizes = [2, 3, 4]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        conv = Conv1D(filters=256, kernel_size=filter_size, padding='same', activation='relu', strides=1)(embedding_layer)\n",
        "        conv = GlobalMaxPooling1D()(conv)\n",
        "        conv_blocks.append(conv)\n",
        "    concat = Concatenate()(conv_blocks)\n",
        "    dropout = Dropout(0.3)(concat)\n",
        "    dense_layer = Dense(64, activation='relu')(dropout)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# 1D CNN Model\n",
        "def create_cnn_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(Conv1D(256, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Bidirectional LSTM Model\n",
        "def create_bidirectional_lstm_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(Bidirectional(LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# CNN-LSTM Model\n",
        "def create_cnn_lstm_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(Conv1D(256, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(128, activation='relu')))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# GRU Model\n",
        "def create_gru_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    model.add(GRU(128, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "models = {\n",
        "    'Transformer': create_transformer_model,\n",
        "    'CRNN': create_crnn_model,\n",
        "    'Hierarchical Attention Network': create_han_model,\n",
        "    'TextCNN': create_textcnn_model,\n",
        "    'LSTM': create_lstm_model,\n",
        "    'CNN': create_cnn_model,\n",
        "    'Bidirectional LSTM': create_bidirectional_lstm_model,\n",
        "    'CNN-LSTM': create_cnn_lstm_model,\n",
        "    'GRU': create_gru_model,\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "nsBRGdVSOZys"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def train_and_evaluate(model, model_name):\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(f\"Training {model_name} model...\")\n",
        "    model.fit(X_train_pad, y_train, validation_split=0.2, epochs=6, batch_size=32, verbose=1)\n",
        "    _, test_acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "    print(f\"{model_name} model accuracy: {test_acc * 100:.2f}%\")\n",
        "    return test_acc\n",
        "\n",
        "model_selection_results = []\n",
        "\n",
        "for model_name, model_builder in models.items():\n",
        "    model = model_builder(embedding_matrix)\n",
        "    test_acc = train_and_evaluate(model, model_name)\n",
        "    model_selection_results.append((model_name, test_acc))\n",
        "\n",
        "best_model_name, _ = max(model_selection_results, key=lambda x: x[1])\n",
        "print(f\"Training and evaluating the best model: {best_model_name}...\")\n",
        "best_model_builder = models[best_model_name]\n",
        "best_model = best_model_builder(embedding_matrix)\n",
        "\n",
        "# Train the best model for more epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "best_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "best_model.fit(X_train_pad, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the best model\n",
        "_, test_acc = best_model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"Best model ({best_model_name}) final accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(f\"{best_model_name}_model.h5\")\n",
        "\n",
        "# Load the saved model to make new predictions\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(f\"{best_model_name}_model.h5\")\n",
        "predictions = loaded_model.predict(X_test_pad)\n",
        "\n",
        "# Generate classification report on the best model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predicted_labels))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, predicted_labels, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "yIKBMgZ4ULLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6086fdf-0f0c-4f43-87fd-77ad74101abb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Transformer model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 28s 91ms/step - loss: 1.2003 - accuracy: 0.6233 - val_loss: 1.1704 - val_accuracy: 0.6203\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 23s 93ms/step - loss: 1.2767 - accuracy: 0.6103 - val_loss: 1.2158 - val_accuracy: 0.6051\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 21s 87ms/step - loss: 263.5996 - accuracy: 0.5867 - val_loss: 223.3103 - val_accuracy: 0.6010\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 20s 82ms/step - loss: 82717.2344 - accuracy: 0.5915 - val_loss: 4152.1406 - val_accuracy: 0.5833\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 20s 83ms/step - loss: 2191.1416 - accuracy: 0.6164 - val_loss: 909.2114 - val_accuracy: 0.6020\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 21s 86ms/step - loss: 934.8096 - accuracy: 0.6210 - val_loss: 582.5222 - val_accuracy: 0.6041\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer model accuracy: 61.81%\n",
            "Training CRNN model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 111s 418ms/step - loss: 1.2767 - accuracy: 0.6223 - val_loss: 1.3009 - val_accuracy: 0.6020\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 106s 431ms/step - loss: 1.2597 - accuracy: 0.6226 - val_loss: 1.3048 - val_accuracy: 0.6020\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 102s 415ms/step - loss: 1.2567 - accuracy: 0.6226 - val_loss: 1.2995 - val_accuracy: 0.6020\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 105s 426ms/step - loss: 1.2580 - accuracy: 0.6226 - val_loss: 1.2980 - val_accuracy: 0.6020\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 103s 416ms/step - loss: 1.2561 - accuracy: 0.6226 - val_loss: 1.2990 - val_accuracy: 0.6020\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 105s 425ms/step - loss: 1.2546 - accuracy: 0.6226 - val_loss: 1.2992 - val_accuracy: 0.6020\n",
            "CRNN model accuracy: 62.21%\n",
            "Training Hierarchical Attention Network model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 9s 17ms/step - loss: 1.1218 - accuracy: 0.6283 - val_loss: 1.0812 - val_accuracy: 0.6385\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 3s 14ms/step - loss: 0.9920 - accuracy: 0.6737 - val_loss: 1.0461 - val_accuracy: 0.6577\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 3s 13ms/step - loss: 0.9188 - accuracy: 0.6975 - val_loss: 1.0447 - val_accuracy: 0.6572\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 3s 11ms/step - loss: 0.8477 - accuracy: 0.7223 - val_loss: 1.0715 - val_accuracy: 0.6562\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 3s 13ms/step - loss: 0.7780 - accuracy: 0.7439 - val_loss: 1.0690 - val_accuracy: 0.6527\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 3s 13ms/step - loss: 0.7054 - accuracy: 0.7710 - val_loss: 1.0781 - val_accuracy: 0.6532\n",
            "Hierarchical Attention Network model accuracy: 65.98%\n",
            "Training TextCNN model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 5s 10ms/step - loss: 1.1268 - accuracy: 0.6328 - val_loss: 1.1104 - val_accuracy: 0.6466\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 2s 7ms/step - loss: 0.9705 - accuracy: 0.6792 - val_loss: 1.1113 - val_accuracy: 0.6405\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 2s 8ms/step - loss: 0.8355 - accuracy: 0.7192 - val_loss: 1.1241 - val_accuracy: 0.6395\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 2s 8ms/step - loss: 0.6877 - accuracy: 0.7648 - val_loss: 1.1420 - val_accuracy: 0.6522\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 2s 8ms/step - loss: 0.5518 - accuracy: 0.8168 - val_loss: 1.2770 - val_accuracy: 0.6365\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 2s 9ms/step - loss: 0.4570 - accuracy: 0.8479 - val_loss: 1.3077 - val_accuracy: 0.6273\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextCNN model accuracy: 62.94%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 230s 909ms/step - loss: 1.2777 - accuracy: 0.6191 - val_loss: 1.3272 - val_accuracy: 0.6020\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 224s 905ms/step - loss: 1.2596 - accuracy: 0.6226 - val_loss: 1.2984 - val_accuracy: 0.6020\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 222s 900ms/step - loss: 1.2589 - accuracy: 0.6226 - val_loss: 1.2982 - val_accuracy: 0.6020\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 222s 898ms/step - loss: 1.2553 - accuracy: 0.6226 - val_loss: 1.2990 - val_accuracy: 0.6020\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 218s 885ms/step - loss: 1.2548 - accuracy: 0.6226 - val_loss: 1.2996 - val_accuracy: 0.6020\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 214s 864ms/step - loss: 1.2560 - accuracy: 0.6226 - val_loss: 1.2966 - val_accuracy: 0.6020\n",
            "LSTM model accuracy: 62.21%\n",
            "Training CNN model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 3s 7ms/step - loss: 1.1427 - accuracy: 0.6285 - val_loss: 1.1068 - val_accuracy: 0.6415\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 1s 5ms/step - loss: 1.0040 - accuracy: 0.6645 - val_loss: 1.1142 - val_accuracy: 0.6334\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 1s 5ms/step - loss: 0.8825 - accuracy: 0.7028 - val_loss: 1.0914 - val_accuracy: 0.6420\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 1s 5ms/step - loss: 0.7432 - accuracy: 0.7530 - val_loss: 1.1646 - val_accuracy: 0.6375\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 1s 5ms/step - loss: 0.6067 - accuracy: 0.7996 - val_loss: 1.2576 - val_accuracy: 0.6385\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 1s 5ms/step - loss: 0.5117 - accuracy: 0.8316 - val_loss: 1.3848 - val_accuracy: 0.6359\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN model accuracy: 62.58%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Bidirectional LSTM model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 430s 2s/step - loss: 1.1554 - accuracy: 0.6229 - val_loss: 1.1435 - val_accuracy: 0.6268\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 419s 2s/step - loss: 1.0653 - accuracy: 0.6429 - val_loss: 1.1273 - val_accuracy: 0.6263\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 417s 2s/step - loss: 1.0479 - accuracy: 0.6532 - val_loss: 1.0807 - val_accuracy: 0.6365\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 420s 2s/step - loss: 1.0066 - accuracy: 0.6622 - val_loss: 1.0824 - val_accuracy: 0.6359\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 417s 2s/step - loss: 0.9743 - accuracy: 0.6755 - val_loss: 1.0702 - val_accuracy: 0.6400\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 416s 2s/step - loss: 0.9418 - accuracy: 0.6865 - val_loss: 1.0807 - val_accuracy: 0.6415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bidirectional LSTM model accuracy: 65.69%\n",
            "Training CNN-LSTM model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 63s 242ms/step - loss: 1.1362 - accuracy: 0.6276 - val_loss: 1.1501 - val_accuracy: 0.6233\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 56s 227ms/step - loss: 1.0324 - accuracy: 0.6512 - val_loss: 1.0966 - val_accuracy: 0.6339\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 60s 242ms/step - loss: 0.9327 - accuracy: 0.6774 - val_loss: 1.1396 - val_accuracy: 0.6132\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 56s 228ms/step - loss: 0.8090 - accuracy: 0.7165 - val_loss: 1.2263 - val_accuracy: 0.6420\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 60s 242ms/step - loss: 0.6710 - accuracy: 0.7683 - val_loss: 1.3014 - val_accuracy: 0.6284\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 56s 228ms/step - loss: 0.5606 - accuracy: 0.8075 - val_loss: 1.3247 - val_accuracy: 0.6116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN-LSTM model accuracy: 61.52%\n",
            "Training GRU model...\n",
            "Epoch 1/6\n",
            "247/247 [==============================] - 203s 793ms/step - loss: 1.2829 - accuracy: 0.6226 - val_loss: 1.3240 - val_accuracy: 0.6020\n",
            "Epoch 2/6\n",
            "247/247 [==============================] - 198s 800ms/step - loss: 1.2589 - accuracy: 0.6226 - val_loss: 1.2968 - val_accuracy: 0.6020\n",
            "Epoch 3/6\n",
            "247/247 [==============================] - 194s 787ms/step - loss: 1.2596 - accuracy: 0.6226 - val_loss: 1.3041 - val_accuracy: 0.6020\n",
            "Epoch 4/6\n",
            "247/247 [==============================] - 197s 796ms/step - loss: 1.2575 - accuracy: 0.6226 - val_loss: 1.2976 - val_accuracy: 0.6020\n",
            "Epoch 5/6\n",
            "247/247 [==============================] - 196s 793ms/step - loss: 1.2550 - accuracy: 0.6226 - val_loss: 1.3091 - val_accuracy: 0.6020\n",
            "Epoch 6/6\n",
            "247/247 [==============================] - 196s 792ms/step - loss: 1.2591 - accuracy: 0.6226 - val_loss: 1.2963 - val_accuracy: 0.6020\n",
            "GRU model accuracy: 62.21%\n",
            "Training and evaluating the best model: Hierarchical Attention Network...\n",
            "Epoch 1/100\n",
            "247/247 [==============================] - 8s 18ms/step - loss: 1.1143 - accuracy: 0.6328 - val_loss: 1.0875 - val_accuracy: 0.6354\n",
            "Epoch 2/100\n",
            "247/247 [==============================] - 3s 11ms/step - loss: 0.9895 - accuracy: 0.6727 - val_loss: 1.0567 - val_accuracy: 0.6552\n",
            "Epoch 3/100\n",
            "247/247 [==============================] - 3s 11ms/step - loss: 0.9186 - accuracy: 0.6941 - val_loss: 1.0500 - val_accuracy: 0.6572\n",
            "Epoch 4/100\n",
            "247/247 [==============================] - 3s 11ms/step - loss: 0.8515 - accuracy: 0.7187 - val_loss: 1.0457 - val_accuracy: 0.6638\n",
            "Epoch 5/100\n",
            "247/247 [==============================] - 3s 12ms/step - loss: 0.7788 - accuracy: 0.7465 - val_loss: 1.0875 - val_accuracy: 0.6501\n",
            "Epoch 6/100\n",
            "247/247 [==============================] - 3s 12ms/step - loss: 0.7069 - accuracy: 0.7693 - val_loss: 1.0708 - val_accuracy: 0.6542\n",
            "Epoch 7/100\n",
            "247/247 [==============================] - 3s 11ms/step - loss: 0.6346 - accuracy: 0.7958 - val_loss: 1.1191 - val_accuracy: 0.6577\n",
            "Best model (Hierarchical Attention Network) final accuracy: 65.61%\n",
            "78/78 [==============================] - 1s 4ms/step\n",
            "Confusion Matrix:\n",
            "[[1423   31   37   16   24    5]\n",
            " [  65   26   19    2   13    1]\n",
            " [ 178   21   33    3   25    1]\n",
            " [  70   23    9   37   12    2]\n",
            " [ 133   37   14    9   86    3]\n",
            " [  70    7    4    4   11   15]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     alarmed       0.73      0.93      0.82      1536\n",
            "    cautious       0.18      0.21      0.19       126\n",
            "   concerned       0.28      0.13      0.18       261\n",
            "  disengaged       0.52      0.24      0.33       153\n",
            "  dismissive       0.50      0.30      0.38       282\n",
            "    doubtful       0.56      0.14      0.22       111\n",
            "\n",
            "    accuracy                           0.66      2469\n",
            "   macro avg       0.46      0.32      0.35      2469\n",
            "weighted avg       0.61      0.66      0.61      2469\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eENaieE7UvNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}