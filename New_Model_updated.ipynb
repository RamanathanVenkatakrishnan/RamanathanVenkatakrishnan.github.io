{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY0sVVaROQ1y",
        "outputId": "421a3193-d2c3-48d7-b33b-a798eaac712a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "Extracting GloVe embeddings...\n",
            "GloVe embeddings are ready!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "# Download and extract GloVe embeddings\n",
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "glove_zip_file = 'glove.6B.zip'\n",
        "\n",
        "if not os.path.exists(glove_zip_file):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    urllib.request.urlretrieve(glove_url, glove_zip_file)\n",
        "\n",
        "glove_embedding_file = 'glove.6B.300d.txt'\n",
        "\n",
        "if not os.path.exists(glove_embedding_file):\n",
        "    print(\"Extracting GloVe embeddings...\")\n",
        "    with zipfile.ZipFile(glove_zip_file, 'r') as z:\n",
        "        z.extractall()\n",
        "\n",
        "print(\"GloVe embeddings are ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAlimUNCkeuA",
        "outputId": "e96a0fcf-a872-4a45-a907-daccafcdac93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et-HSt3r_ik6",
        "outputId": "1d8a0041-5276-4219-ca54-0d5b48ce599c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH0BoMSPQu7o",
        "outputId": "89d6860e-3733-4b31-df04-e795ff7b40c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and preprocess the data\n",
        "data = load_and_preprocess_data('/content/drive/MyDrive/new_data.csv')\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    \n",
        "    # Remove non-ASCII characters and convert to lowercase\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode().lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords and stem the remaining words\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    \n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def load_and_preprocess_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    data['Text'] = data['Text'].apply(clean_text)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oS9ASn87OVNe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define num_classes and encode labels\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "x9fSwNy6OVPz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenize and pad sequences\n",
        "max_features = 10000\n",
        "max_length = 100\n",
        "\n",
        "# Custom filter to remove special characters\n",
        "custom_filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=max_features,\n",
        "    filters=custom_filters,\n",
        "    lower=True,\n",
        "    split=\" \",\n",
        "    char_level=False, # Set to True for character-level tokenization\n",
        "    oov_token=\"<OOV>\", # Out-of-vocabulary token for words not in the training data\n",
        ")\n",
        "\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Save tokenizer for future use (e.g., when preprocessing new data)\n",
        "import pickle\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "w6Y4XJH3OVSU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load GloVe pre-trained word embeddings\n",
        "def load_glove_embeddings(embedding_file, tokenizer):\n",
        "    embeddings_index = {}\n",
        "    with open(embedding_file, encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = load_glove_embeddings('glove.6B.300d.txt', tokenizer)\n"
      ],
      "metadata": {
        "id": "QmE2h2G3OVUm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Build and train different deep learning models\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, TimeDistributed, Flatten, GRU, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, TimeDistributed, Flatten, MaxPooling1D, Concatenate, Attention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "# Transformer model\n",
        "def create_transformer_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    transformer_block = MultiHeadAttention(num_heads=8, key_dim=300)(embedding_layer, embedding_layer)\n",
        "    x = transformer_block\n",
        "    for _ in range(2):\n",
        "        x = MultiHeadAttention(num_heads=8, key_dim=300)(x, x)\n",
        "    \n",
        "    pooling_layer = GlobalAveragePooling1D()(x)\n",
        "    dense_layer = Dense(64, activation='relu')(pooling_layer)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "  \n",
        "\n",
        "# CRNN model\n",
        "def create_crnn_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    conv1d_layer = Conv1D(256, 5, activation='relu')(embedding_layer)\n",
        "    max_pooling_layer = MaxPooling1D(pool_size=2)(conv1d_layer)\n",
        "    gru_layer1 = GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(max_pooling_layer)\n",
        "    gru_layer2 = GRU(128, dropout=0.3, recurrent_dropout=0.3)(gru_layer1)\n",
        "    dense_layer1 = Dense(64, activation='relu')(gru_layer2)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer1)\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Hierarchical Attention Network (HAN) model\n",
        "def create_han_model(embedding_matrix):\n",
        "    # ... (same as before, including the many_to_one_lstm function)\n",
        "\n",
        "    sent_input = Input(shape=(None, max_length))\n",
        "    sent_embedding = TimeDistributed(word_encoder)(sent_input)\n",
        "    sent_attention = many_to_one_lstm(sent_embedding)\n",
        "    dense_layer = Dense(64, activation='relu')(sent_attention)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "    return Model(inputs=sent_input, outputs=output_layer)\n",
        "\n",
        "# TextCNN model\n",
        "def create_textcnn_model(embedding_matrix):\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    conv_blocks = []\n",
        "    filter_sizes = [2, 3, 4]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        conv = Conv1D(filters=256, kernel_size=filter_size, padding='same', activation='relu', strides=1)(embedding_layer)\n",
        "        conv = GlobalMaxPooling1D()(conv)\n",
        "        conv_blocks.append(conv)\n",
        "    concat = Concatenate()(conv_blocks)\n",
        "    dropout = Dropout(0.3)(concat)\n",
        "    dense_layer = Dense(64, activation='relu')(dropout)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "# LSTM Model\n",
        "def create_lstm_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# 1D CNN Model\n",
        "def create_cnn_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(Conv1D(256, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Bidirectional LSTM Model\n",
        "def create_bidirectional_lstm_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(Bidirectional(LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
        "    model.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# CNN-LSTM Model\n",
        "def create_cnn_lstm_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(Conv1D(256, 5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(128, activation='relu')))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# GRU Model\n",
        "def create_gru_model(embedding_matrix):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    model.add(GRU(128, dropout=0.3, recurrent_dropout=0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "models = {\n",
        "    'Transformer': create_transformer_model,\n",
        "    'CRNN': create_crnn_model,\n",
        "    'Hierarchical Attention Network': create_han_model,\n",
        "    'TextCNN': create_textcnn_model,\n",
        "    'LSTM': create_lstm_model,\n",
        "    'CNN': create_cnn_model,\n",
        "    'Bidirectional LSTM': create_bidirectional_lstm_model,\n",
        "    'CNN-LSTM': create_cnn_lstm_model,\n",
        "    'GRU': create_gru_model,\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "nsBRGdVSOZys"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def train_and_evaluate(model, model_name):\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(f\"Training {model_name} model...\")\n",
        "    model.fit(X_train_pad, y_train, validation_split=0.3, epochs=10, batch_size=42, verbose=1)\n",
        "    _, test_acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "    print(f\"{model_name} model accuracy: {test_acc * 100:.2f}%\")\n",
        "    return test_acc\n",
        "\n",
        "model_selection_results = []\n",
        "\n",
        "for model_name, model_builder in models.items():\n",
        "    model = model_builder(embedding_matrix)\n",
        "    test_acc = train_and_evaluate(model, model_name)\n",
        "    model_selection_results.append((model_name, test_acc))\n",
        "    print(f\"Finished training {model_name}\\n\")\n",
        "\n",
        "best_model_name, _ = max(model_selection_results, key=lambda x: x[1])\n",
        "print(f\"Training and evaluating the best model: {best_model_name}...\")\n",
        "best_model_builder = models[best_model_name]\n",
        "best_model = best_model_builder(embedding_matrix)\n",
        "\n",
        "# Train the best model for more epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "best_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "best_model.fit(X_train_pad, y_train, validation_split=0.2, epochs=100, batch_size=32, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the best model\n",
        "_, test_acc = best_model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"Best model ({best_model_name}) final accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Save the best model\n",
        "best_model.save(f\"{best_model_name}_model.h5\")\n",
        "\n",
        "# Load the saved model to make new predictions\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(f\"{best_model_name}_model.h5\")\n",
        "predictions = loaded_model.predict(X_test_pad)\n",
        "\n",
        "# Generate classification report on the best model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predicted_labels))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, predicted_labels, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "yIKBMgZ4ULLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3797d2f0-274c-444d-83ed-1ff4c267d442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Transformer model...\n",
            "Epoch 1/10\n",
            "165/165 [==============================] - 1454s 9s/step - loss: 1.2438 - accuracy: 0.6190 - val_loss: 1.1867 - val_accuracy: 0.6168\n",
            "Epoch 2/10\n",
            " 90/165 [===============>..............] - ETA: 9:23 - loss: 1.1503 - accuracy: 0.6238"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization, Dropout, Layer, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "class MultiHeadSelfAttention(Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = Dense(embed_dim)\n",
        "        self.key_dense = Dense(embed_dim)\n",
        "        self.value_dense = Dense(embed_dim)\n",
        "        self.combine_heads = Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "\n",
        "def create_transformer_model(embedding_matrix):\n",
        "    num_layers = 2\n",
        "    num_heads = 8\n",
        "    dff = 128\n",
        "    dropout_rate = 0.1\n",
        "\n",
        "    def point_wise_feed_forward_network(d_model, dff):\n",
        "        return tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "            Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "        ])\n",
        "\n",
        "    class TransformerBlock(Layer):\n",
        "        def __init__(self, d_model, num_heads, dff, rate=dropout_rate):\n",
        "            super(TransformerBlock, self).__init__()\n",
        "\n",
        "            self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
        "            self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "            self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "            self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "            self.dropout1 = Dropout(rate)\n",
        "            self.dropout2 = Dropout(rate)\n",
        "\n",
        "        def call(self, x, training):\n",
        "            attn_output = self.mha(x)  # (batch_size, seq_len, d_model)\n",
        "            attn_output = self.dropout1(attn_output, training=training)\n",
        "            out1 = self.layernorm1(x + attn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "            ffn_output = self.ffn(out1)  # (batch_size, seq_len, d_model)\n",
        "            ffn_output = self.dropout2(ffn_output, training=training)\n",
        "            out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "            return out2\n",
        "\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    embedding_layer = Embedding(len(embedding_matrix), 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(input_layer)\n",
        "    x = embedding_layer\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerBlock(300, num_heads, dff, dropout_rate)(x)\n",
        "\n",
        "    pooling_layer = GlobalAveragePooling1D()(x)\n",
        "    dense_layer = Dense(64, activation='relu')(pooling_layer)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "    return Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "6qNVwRGhmSDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save best model\n",
        "best_model.save(f\"{best_model_name}_model.h5\")\n"
      ],
      "metadata": {
        "id": "TD_vZWodUvAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "71b6cddd-413b-46f9-872d-8874aa2248de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d76d2dee095f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#save best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{best_model_name}_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load saved model to make new predictions on best model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(f\"{best_model_name}_model.h5\")\n",
        "predictions = loaded_model.predict(X_test_pad)\n"
      ],
      "metadata": {
        "id": "TC_lvAGyUvIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate classification report on best model\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_encoded, predicted_labels))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_encoded, predicted_labels, target_names=encoder.classes_))\n"
      ],
      "metadata": {
        "id": "B_L8Fb7HUvKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eENaieE7UvNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}